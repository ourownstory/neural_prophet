http://neuralprophet.com/hyperparameter-selection/

NeuralProphet有一些超参数需要用户指定。如果没有指定，将使用这些超参数的默认值。如下。

| Parameter             | Default Value |
| --------------------- | ------------- |
| `growth`              | linear        |
| `changepoints`        | None          |
| `n_changepoints`      | 5             |
| `changepoints_range`  | 0.8           |
| `trend_reg`           | 0             |
| `trend_reg_threshold` | False         |
| `yearly_seasonality`  | auto          |
| `weekly_seasonality`  | auto          |
| `daily_seasonality`   | auto          |
| `seasonality_mode`    | additive      |
| `seasonality_reg`     | None          |
| `n_forecasts`         | 1             |
| `n_lags`              | 0             |
| `num_hidden_layers`   | 0             |
| `d_hidden`            | None          |
| `ar_sparsity`         | None          |
| `learning_rate`       | None          |
| `epochs`              | None          |
| `batch_size`          | None          |
| `loss_func`           | Huber         |
| `train_speed`         | None          |
| `normalize_y`         | auto          |
| `impute_missing`      | True          |



## 预测范围

`n_forecasts`是预测范围的大小。默认值为1，表示模型预测未来一步。

## 自回归

`n_lags`定义AR-Net是否启用(如果`n_lags`>0)。如果可能的话，通常建议`n_lags'的值大于``n_forecasts`。FFNNs最好至少遇到过去的`n_forecast`长度，以便预测未来的`n_forecast`。因此，`n_lags`决定应该考虑到过去多远的自动递减依赖性。这可以是根据专业知识或经验分析选择的一个值。

## 模型训练相关参数

NeuralProphet采用随机梯度下降法进行拟合--更准确地说，是采用AdamW优化器和One-Cycle策略。如果没有指定参数`learning_rate`，则进行学习率范围测试以确定最佳学习率。`epochs`和`loss_func`是另外两个直接影响模型训练过程的参数。如果没有定义，这两个参数会根据数据集大小自动设置。它们的设置方式是将训练步骤总数控制在1000到4000左右。

如果看起来模型对训练数据过度拟合（实时损失图在此很有用），可以减少 `epochs` 和 `learning_rate`，并有可能增加 `batch_size`。如果是低拟合，可以增加`epochs` 和`learning_rate` 的数量，并有可能减少`batch_size` 。

默认的损失函数是 "Huber "损失，该函数被认为对离群值具有鲁棒性。但是，您可以自由选择标准的 "MSE "或任何其他PyTorch `torch.nn.modules.loss`损失函数。

## 增加模型的深度

`num_hidden_layers`定义了整个模型中使用的FFNN的隐藏层数。这包括AR-Net和滞后回归项的FNNN。默认值为0，意味着FFNNs将只有一个大小为`n_forecasts`的最后一层。增加更多的层数会导致复杂度增加，也会因此增加计算时间。然而，增加的隐藏层数可以帮助建立更复杂的关系，特别是对滞后回归者有用。为了在计算复杂性和提高精度之间进行权衡，建议将 `num_hidden_layers` 设置在1-2之间。然而，在大多数情况下，完全没有隐藏层可以获得足够好的性能。

`d_hidden`是隐藏层的单位数。只有在指定了 "num_hidden_layers "的情况下才会考虑，否则忽略。如果没有指定，`d_hidden`的默认值是(`n_lags` + `n_forecasts`)。如果手动调整，建议的做法是在`n_lags`和`n_forecasts`之间为`d_hidden`设置一个值。还需要注意的是，在当前的实现中，NeuralProphet为所有的隐藏层设置了相同的`d_hidden`。

## 数据预处理相关参数

`normalize_y` 是关于在建模前对时间序列进行缩放。默认情况下，NeuralProphet会对时间序列进行（soft）最小-最大的归一化。如果序列值波动很大，归一化可以帮助模型训练过程。然而，如果序列没有这样的缩放，用户可以关闭这个功能或选择其他归一化。

`impute_missing`是关于在一个给定的序列中推算缺失值。与Prophet类似，NeuralProphet也可以在没有AR-Net的回归模式下处理缺失值。然而，当需要捕获自相关时，有必要对缺失值进行估算，因为这时建模就变成了一个有序的问题。在大多数情况下，让这个参数处于默认状态可以完美地完成工作。

## 趋势相关参数

你可以在[`example_notebooks/trend_peyton_manning.ipynb`](https://github.com/ourownstory/neural_prophet/blob/master/example_notebooks/trend_peyton_manning.ipynb)找到一个实践的例子。

如果趋势的灵活性主要由`n_changepoints`控制，它设定了趋势率可能变化的点数。此外，可以通过将`trend_reg`设置为大于零的值来规范趋势率的变化。

这是一个有用的功能，可以用来自动检测相关的变化点。

`changepoints_range`控制用于拟合趋势的训练数据的范围。默认值为0.8，意味着在最后20%的训练数据中不设置变化点。

如果提供了一个`changepoints` 列表，`n_changepoints` 和 `changepoints_range` 将被忽略。这个列表用于设置允许趋势率变化的日期。

`n_changepoints`是沿系列选择的趋势变化点的数量。默认值为5。

## 季节性相关参数

`yearly_seasonality`、`weekly_seasonality` 和 `daily_seasonality` 是关于要模拟的季节成分。例如，如果你使用温度数据，你可能可以选择每天和每年。例如，使用使用地铁的乘客数量更可能有一个每周的季节性。将这些季节性设置在默认的`auto`模式下，可以让NeuralProphet根据可用数据的多少来决定包括哪些季节性。例如，如果可用数据少于两年，则不会考虑年季节性。同样，如果可用数据少于两周，每周的季节性将不被考虑等等。然而，如果用户确定系列不包括年、周或日季节性，因此模型不应该被这些成分扭曲，他们可以通过设置相应的成分为`False`来明确关闭它们。除此之外，参数 `yearly_seasonality`、`weekly_seasonality` 和 `daily_seasonality` 也可以设置为各自季节性的傅里叶项数。默认值为年6，周4和日6。用户可以将其设置为任何他们想要的数字。如果每年的项数为6，那么实际上每年季节性的傅里叶项总数为12(6*2)，以适应正弦和余弦项。增加Fourier项的数量可以使模型能够捕捉相当复杂的季节性模式。然而，与 `num_hidden_layers`类似，这也会增加模型的复杂性。用户可以通过观察最终的分量图来了解最佳的Fourier项数。默认的`seasonality_mode`是加法。这意味着在季节性方面，序列中没有异方差。然而，如果序列包含明显的方差，季节性波动与趋势成正比，则可以将`seasonality_mode` 设置为乘法。

## 正则化相关参数

NeuralProphet还包含一些正则化参数，用于控制模型系数，并将稀疏性引入模型。这也有助于避免模型对训练数据的过度拟合。对于`seasonality_reg`，在0.1-1范围内的小值允许拟合大的季节性波动，而在1-100范围内的大值则会对傅里叶系数施加较重的惩罚，从而抑制季节性。对于 "ar_sparsity"，预计值在0-1的范围内，0会引起完全的稀疏性，而1则完全不需要正则化。`ar_sparsity`和`n_lags`可以用于数据探索和特征选择。由于AR-Net的可扩展性，你可以使用更多的滞后数，并利用稀疏性来识别过去时间步长对预测精度的重要影响。对于 `future_regressor_regularization`、`event_regularization` 和 country_holiday_regularization，其值可以设置在0-1之间，与 `ar_sparsity`的概念相同。可以根据各个回归因子和事件需要更多的抑制，设置不同的正则化参数。
